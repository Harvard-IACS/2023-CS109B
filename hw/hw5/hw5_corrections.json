{"**1.3 - Input Sequence and Target Pairs**": "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6e6fa; border-color: #d8bfd8; border-width: 1px; border-radius: 3px; padding: 10px;\">\n\n**1.3 - Input Sequence and Target Pairs**\n\nNow that we have our character to integer mappings, we can begin creating our input sequence and target pairs. This process involves breaking up our text into fixed-length sequences, where each sequence is used to predict the character that follows that sequence in the text. \n\nYour task is to implement the code to generate these input sequence and target pairs using the following hyperparameters:\n- `SEQ_LEN`: the length of each input sequence (default 100)\n- `STEP`: the stride between each sequence (default 1)\n    \nFor example, suppose our input text is \"the owl and the pussy-cat went to sea\". If `SEQ_LEN` is 5 and `STEP` is 2, we will create the following input sequences and target pairs:\n\n| input sequence | target pair |\n|---|---|\n| `['t', 'h', 'e', ' ', 'o']` | `'w'` |\n| `['e', ' ', 'o', 'w', 'l']` | `' '` |\n| `['o', 'w', 'l', ' ', 'a']` | `'n'` |\n| `['l', ' ', 'a', 'n', 'd']` | `' '` |\n| `['a', 'n', 'd', ' ', 't']` | `'h'` |\n| `['d', ' ', 't', 'h', 'e']` | `' '` |\n| `['t', 'h', 'e', ' ', 'p']` | `'u'` |\n| `['e', ' ', 'p', 'u', 's']` | `'s'` |\n| `['p', 'u', 's', 's', 'y']` | `'-'` |\n| `['s', 's', 'y', '-', 'c']` | `'a'` |\n| `['y', '-', 'c', 'a', 't']` | `' '` |\n| `['c', 'a', 't', ' ', 'w']` | `'e'` |\n| `['t', ' ', 'w', 'e', 'n']` | `'t'` |\n| `['w', 'e', 'n', 't', ' ']` | `'t'` |\n| `['n', 't', ' ', 't', 'o']` | `' '` |\n| `[' ', 't', 'o', ' ', 's']` | `'e'` |\n\nYou will need to create two numpy arrays: `x` and `y`. `x` should be an integer numpy array of shape (num_sequences, `SEQ_LEN`), where num_sequences is the total number of input sequences. Each element of `x` should be an integer representing the character at that position in the input sequence. `y` should be an integer numpy array of shape (num_sequences,), where each element is the integer representation of the target character for the corresponding input sequence.\n\nAfter you have generated `x` and `y`, print their shapes and data types.\n</div>", "# Display info of input sequences and targets": "# Display info of input sequences and targets\nprint(\"Number of sequences:\", len(x))\nprint(\"x shape:\", x.shape)\nprint(\"y shape:\", y.shape)", "**1.6 - Temperature**": "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6e6fa; border-color: #d8bfd8; border-width: 1px; border-radius: 3px; padding: 10px;\">\n\n**1.6 - Temperature**\n    \nWhen generating text with a language model, we want to balance between predictability and randomness. We use the notion of temperature to control the amount of randomness in the text. Here's how it works:\n\nGiven a probability distribution of the next character, the temperature is applied as a scaling factor to the log probabilities. This is done to adjust the shape of the distribution. A higher temperature means a flatter distribution, where all characters are equally likely, and a lower temperature means a sharper distribution, where the most probable character has a higher probability.\n\nYour task is to implement a function called `sample` which takes in three arguments: `preds` - the probability distribution of the next character, `temperature` - the temperature to be applied, and `eps` - a small value to prevent us from taking the log of zero. It should return an integer corresponding to the index of the sampled character. Here's the formula for the sampling probabilities when applying temperature:\n\n$$\n\text{probas} = \frac{\\exp{\big[\\log{(\text{preds}_i)}\\ /\\ T\big]}}{\\sum_j{\\exp{\big[\\log{(\text{preds}_j)}\\ /\\ T\big]}}}\n$$\n \nYou will likely want to make use of numpy's logarithm and exponential functions as well as `np.random.multinomial` to implement `sample`.\n    \n</br>\n\n**Note:** to avoid issues with numerical precission it may be necessary to convert `preds` to type np.float64. Commenting out the `keras.mixed_precision.set_global_policy(\"mixed_float16\")` line at the bottom of the imports cell at the beginning of the notebook may also help prevent precision issues, Ttough doing so will likely cause your notebook to run more slowly.\n</div>", "**2.1.7 - Pad X**": "<div class=\"alert alert-info\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 10px;\">\n\n**2.1.7 - Pad X**\n    \nApply the keras ```pad_sequences``` function to standardize the length of input sequences and \"pre\" pad them with `0`. You should retrieve a matrix with all padded sentences and length equal to ```max_len``` previously computed. The dimensionality should therefore be equal to ```[# of sentences, max_len]```. Run the provided cell to print your results. ```X[1]``` should now look like this:\n\n```    \n[    0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0  8194 27728 31034 33290 22578\n 33465 23724 16666 33465 31143 31320 28268 27701 33247 28647 16053    22\n 16916 17350  7925 32880 32986 18239 23556    25]\n```\n\n</div>", "def visualize_B_I(": "# Helper functions\n# Note: updated function requires y_test as an argument\ndef visualize_B_I(pca_result, y_test):\n    color = ['r', 'C1', 'y', 'C3', 'b', 'g', 'm', 'orange']\n    category = y_test.flatten()\n    unique_categories = np.unique(category)\n    fig, ax = plt.subplots(1,2) \n    fig.set_size_inches(12,6)\n    for i in range(2):\n        for cat in unique_categories[8*i:8*(i+1)]:\n            indices = np.where(category==cat+1)[0]\n            ax[i].scatter(pca_result[indices,0], pca_result[indices, 1], label=idx2tag[cat+1],s=2,color=color[cat-8*i],alpha=0.5)\n        legend = ax[i].legend(markerscale=3)\n        legend.get_frame().set_facecolor('w') \n        ax[i].set_xlabel(\"PC1\")\n        ax[i].set_ylabel(\"PC2\")\n    fig.suptitle(\"visualization of hidden feature on reduced dimension by PCA\")\n\n\ndef get_hidden_output_PCA(model, X_test, y_test, layer_index, out_dimension):\n    latent_space_extractor = Model(model.inputs, model.layers[layer_index].output)\n    hidden_feature=np.array(latent_space_extractor([X_test]))\n\n    hidden_feature=hidden_feature.reshape(-1,out_dimension)\n    \n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(hidden_feature)\n    print(f'Variance explained by PCA: {np.sum(pca.explained_variance_ratio_):.2%}')\n    return pca_result"}